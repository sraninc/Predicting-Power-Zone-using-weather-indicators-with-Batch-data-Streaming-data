{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b02127a-d5a5-48d5-bc86-d79404cf2bc7",
   "metadata": {},
   "source": [
    "# ST 590 (Final Project) Sneha Rani"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d2f93c-4631-48da-b5e5-0d1f1a620148",
   "metadata": {},
   "source": [
    "In this project, we will be working with a dataset downloaded from UCI machine learning repository and then later modified for this project purposes. The data is based on relating power consumption from different zones of Tetouan city to various factors like time of day, temperature, and humidity. Here is an overview of the dataset:\n",
    "\n",
    "| Column name | Description | Data Type |\n",
    "| --- | --- | --- |\n",
    "| Temperature | Weather Temperature of Tetouan city | (Double) |\n",
    "| Humidity | Weather Humidity of Tetouan city | (Double) |\n",
    "| Wind_Speed | Wind Speed of Tetouan city | (Double) |\n",
    "| General_Diffuse_Flows | General diffuse flows | (Double) |\n",
    "| Diffuse_Flows | Diffuse flows | (Double) |\n",
    "| Power_Zone_1 | power consumption of zone 1 of Tetouan city | (Double) |\n",
    "| Power_Zone_2 | power consumption of zone 2 of Tetouan city | (Double) |\n",
    "| Power_Zone_3 | power consumption of zone 3 of Tetouan city | (Double) |\n",
    "| Month | Months in a year | (Integer) |\n",
    "| Hour | Hours in a day | (Float) |\n",
    "\n",
    "This project is divided into two parts. \n",
    "- In this part, we are dealing with batch/static data. We will first download the data and read as a spark SQL dataframe. After that, we will summarize the data using various numerical summary methods including correlation, means, standard deviation etc. This will give us some basic information about our data. Then we will create a pipeline for a series of transformations. Then we will fit Elastic Net model with some regularization parameters, the best parameters will be chosen using cross validation. Finally, we will estimate the training RMSE on this data.\n",
    "- Second part of the project is based on streaming data. The streaming data will be randomly sampled from three rows of the dataset iteratively. This streaming data will then be used for model transformation and making predictions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ecc0b5-6879-407a-8fc9-6acf8fc540ce",
   "metadata": {},
   "source": [
    "## Part-1: Model Fitting\n",
    "\n",
    "We will begin with reading the dataset, importing required libraries, and finally starting a spark session to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4437fee-aaba-47cb-8b92-3d0d5c2ef69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind_Speed</th>\n",
       "      <th>General_Diffuse_Flows</th>\n",
       "      <th>Diffuse_Flows</th>\n",
       "      <th>Power_Zone_1</th>\n",
       "      <th>Power_Zone_2</th>\n",
       "      <th>Power_Zone_3</th>\n",
       "      <th>Month</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.559</td>\n",
       "      <td>73.8</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.119</td>\n",
       "      <td>34055.69620</td>\n",
       "      <td>16128.87538</td>\n",
       "      <td>20240.96386</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.414</td>\n",
       "      <td>74.5</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.085</td>\n",
       "      <td>29814.68354</td>\n",
       "      <td>19375.07599</td>\n",
       "      <td>20131.08434</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.313</td>\n",
       "      <td>74.5</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.100</td>\n",
       "      <td>29128.10127</td>\n",
       "      <td>19006.68693</td>\n",
       "      <td>19668.43373</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.121</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.096</td>\n",
       "      <td>28228.86076</td>\n",
       "      <td>18361.09422</td>\n",
       "      <td>18899.27711</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.921</td>\n",
       "      <td>75.7</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.085</td>\n",
       "      <td>27335.69620</td>\n",
       "      <td>17872.34043</td>\n",
       "      <td>18442.40964</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Temperature  Humidity  Wind_Speed  General_Diffuse_Flows  Diffuse_Flows  \\\n",
       "0        6.559      73.8       0.083                  0.051          0.119   \n",
       "1        6.414      74.5       0.083                  0.070          0.085   \n",
       "2        6.313      74.5       0.080                  0.062          0.100   \n",
       "3        6.121      75.0       0.083                  0.091          0.096   \n",
       "4        5.921      75.7       0.081                  0.048          0.085   \n",
       "\n",
       "   Power_Zone_1  Power_Zone_2  Power_Zone_3  Month  Hour  \n",
       "0   34055.69620   16128.87538   20240.96386      1   0.0  \n",
       "1   29814.68354   19375.07599   20131.08434      1   0.0  \n",
       "2   29128.10127   19006.68693   19668.43373      1   0.0  \n",
       "3   28228.86076   18361.09422   18899.27711      1   0.0  \n",
       "4   27335.69620   17872.34043   18442.40964      1   0.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = pd.read_csv(\"power_ml_data.csv\") #read csv file\n",
    "data=data.astype({'Hour':'float'}) #change integer to float type\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c869bb-8b6a-4181-a7f4-0f720e1e9184",
   "metadata": {},
   "source": [
    "Now that we have the data downloaded, we want to first define the datatypes that are compatible with spark so we will define a schema for this dataset with appropriate datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb8d081c-2203-4193-ba93-3e26c6a1061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Spark libraries\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType, DoubleType, FloatType\n",
    "\n",
    "# Setup the expected schema\n",
    "expected_schema = StructType([\n",
    "    StructField(\"Temperature\", DoubleType(), True),\n",
    "    StructField(\"Humidity\", DoubleType(), True),\n",
    "    StructField(\"Wind_Speed\", DoubleType(), True),\n",
    "    StructField(\"General_Diffuse_Flows\", DoubleType(), True),\n",
    "    StructField(\"Diffuse_Flows\", DoubleType(), True),\n",
    "    StructField(\"Power_Zone_1\", DoubleType(), True),\n",
    "    StructField(\"Power_Zone_2\", DoubleType(), True),\n",
    "    StructField(\"Power_Zone_3\", DoubleType(), True),\n",
    "    StructField(\"Month\", IntegerType(), True),\n",
    "    StructField(\"Hour\", DoubleType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a450b1f-bdf4-412c-afe2-9c948bf7a3d6",
   "metadata": {},
   "source": [
    "Now that we have our schema ready, we can create a spark SQL dataframe with this schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc1eac2-c12c-43a9-8d3e-4c02a02a75ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+---------------------+-------------+------------+------------+------------+-----+----+\n",
      "|Temperature|Humidity|Wind_Speed|General_Diffuse_Flows|Diffuse_Flows|Power_Zone_1|Power_Zone_2|Power_Zone_3|Month|Hour|\n",
      "+-----------+--------+----------+---------------------+-------------+------------+------------+------------+-----+----+\n",
      "|      6.559|    73.8|     0.083|                0.051|        0.119|  34055.6962| 16128.87538| 20240.96386|    1| 0.0|\n",
      "|      6.414|    74.5|     0.083|                 0.07|        0.085| 29814.68354| 19375.07599| 20131.08434|    1| 0.0|\n",
      "|      6.313|    74.5|      0.08|                0.062|          0.1| 29128.10127| 19006.68693| 19668.43373|    1| 0.0|\n",
      "|      6.121|    75.0|     0.083|                0.091|        0.096| 28228.86076| 18361.09422| 18899.27711|    1| 0.0|\n",
      "|      5.921|    75.7|     0.081|                0.048|        0.085|  27335.6962| 17872.34043| 18442.40964|    1| 0.0|\n",
      "+-----------+--------+----------+---------------------+-------------+------------+------------+------------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas dataframe to Spark SQL dataframe \n",
    "spark_data = spark.createDataFrame(data, schema=expected_schema)\n",
    "spark_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f975fc2-c46b-49a0-ad59-64659f77c899",
   "metadata": {},
   "source": [
    "Our spark SQL dataframe is ready! <br>\n",
    "<br>\n",
    "Now let's look at the summary of this dataset, to get an overview of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28571e4-297c-462d-9057-00bb26fe4ce8",
   "metadata": {},
   "source": [
    "### Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c0d0b9-59f5-40b9-9b87-e64bec1a2710",
   "metadata": {},
   "source": [
    "Here is the summary of the dataset. \n",
    "- Rows of the summary table contain `count`, `mean`, `standard deviations`, `minimum value`, and `maximum value`.\n",
    "- Columns of the summary table contain all the variables of the dataset\n",
    "<br>\n",
    "<br>\n",
    "(You might notice that the summary is created in two parts. This is done only for the readability purposes and to make the results look clean.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8fabdfc-e3eb-4e36-b3a2-7961e77fc881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+---------------------+------------------+------------------+\n",
      "|summary|       Temperature|          Humidity|        Wind_Speed|General_Diffuse_Flows|     Diffuse_Flows|      Power_Zone_1|\n",
      "+-------+------------------+------------------+------------------+---------------------+------------------+------------------+\n",
      "|  count|             47174|             47174|             47174|                47174|             47174|             47174|\n",
      "|   mean|18.813219803281438| 68.28839827023359|1.9616214016194993|   182.53118043837654| 74.98721140882624|32335.168690495833|\n",
      "| stddev| 5.813341359553941|15.560330479134192|2.3493511404671956|   264.43185588803766|124.25614632084834| 7130.013305333635|\n",
      "|    min|             3.247|             11.34|              0.05|                0.004|             0.011|        13895.6962|\n",
      "|    max|             40.01|              94.8|             6.483|               1163.0|             936.0|       52146.85905|\n",
      "+-------+------------------+------------------+------------------+---------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_data.select(\"Temperature\", \"Humidity\", \"Wind_Speed\",\"General_Diffuse_Flows\",\"Diffuse_Flows\",\\\n",
    "                 \"Power_Zone_1\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2deaaf2-a0d7-4c3b-a3e1-dbd65e927633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------------+------------------+\n",
      "|summary|      Power_Zone_2|      Power_Zone_3|            Month|              Hour|\n",
      "+-------+------------------+------------------+-----------------+------------------+\n",
      "|  count|             47174|             47174|            47174|             47174|\n",
      "|   mean| 21027.20497598019|17831.197607816728|6.510599058803578|11.488383431551279|\n",
      "| stddev|5199.7871528123005|6622.5904698693585|3.437367004368705| 6.921920450693015|\n",
      "|    min|       8560.081466|        5935.17407|                1|               0.0|\n",
      "|    max|       37408.86076|       47598.32636|               12|              23.0|\n",
      "+-------+------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_data.select(\"Power_Zone_2\", \"Power_Zone_3\",\"Month\",\"Hour\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8320bf4-fe8b-44b9-8104-4c32f22841dd",
   "metadata": {},
   "source": [
    "### Correlation Matrix\n",
    "\n",
    "Now let us look at the correlation between all the numeric variables. For this purpose, we will create a vector for all the variables using `VectorAssembler()`. This vectorized column will then be used to compute correlation. Since, the output is a dense matrix, we will then convert this dense matrix into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ed7f89f-da67-47d8-a4cd-822c5d34b35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+---------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|        Temperature|            Humidity|          Wind_Speed|General_Diffuse_Flows|       Diffuse_Flows|        Power_Zone_1|        Power_Zone_2|        Power_Zone_3|               Month|                Hour|\n",
      "+-------------------+--------------------+--------------------+---------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                1.0| -0.4601429412885585|  0.4764210955803574|   0.4596021000162658| 0.19562514523387217|  0.4414462233573754|  0.3843011135514114| 0.49075247369111336|  0.2848137789728194|  0.1991337035789466|\n",
      "|-0.4601429412885585|                 1.0|-0.13612147371171135|  -0.4672824842880118|-0.25804180556209755|-0.28909012471237544| -0.2970193920319654|-0.23422766449272228|-0.01676211154676419|-0.24420351156941025|\n",
      "| 0.4764210955803574|-0.13612147371171135|                 1.0|  0.13230382191546244|-7.27182719954412...| 0.16632237305298822| 0.14633773695530827| 0.27911206696644714|  0.1684914415582904|0.004289118670360103|\n",
      "| 0.4596021000162658| -0.4672824842880118| 0.13230382191546244|                  1.0|  0.5645303710201613| 0.18999389896220842| 0.15879848656361106| 0.06494174322961248|-0.02079310613394048|  0.1311706624591642|\n",
      "|0.19562514523387217|-0.25804180556209755|-7.27182719954412...|   0.5645303710201613|                 1.0| 0.08288522829466122|0.047378696387671705|-0.03676087244332553|-0.13024892956930414| 0.13225696715088406|\n",
      "| 0.4414462233573754|-0.28909012471237544| 0.16632237305298822|  0.18999389896220842| 0.08288522829466122|                 1.0|  0.8346940579474813|  0.7506555678809349|-0.00642852925784...|  0.7281175963531654|\n",
      "| 0.3843011135514114| -0.2970193920319654| 0.14633773695530827|  0.15879848656361106|0.047378696387671705|  0.8346940579474813|                 1.0|  0.5723439832303417| 0.31836841946276645|  0.6637546743696425|\n",
      "|0.49075247369111336|-0.23422766449272228| 0.27911206696644714|  0.06494174322961248|-0.03676087244332553|  0.7506555678809349|  0.5723439832303417|                 1.0|-0.23297815366151126| 0.45480639482871665|\n",
      "| 0.2848137789728194|-0.01676211154676419|  0.1684914415582904| -0.02079310613394048|-0.13024892956930414|-0.00642852925784...| 0.31836841946276645|-0.23297815366151126|                 1.0|-4.19363425066785...|\n",
      "| 0.1991337035789466|-0.24420351156941025|0.004289118670360103|   0.1311706624591642| 0.13225696715088406|  0.7281175963531654|  0.6637546743696425| 0.45480639482871665|-4.19363425066785...|                 1.0|\n",
      "+-------------------+--------------------+--------------------+---------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.linalg import DenseMatrix, Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create vector for all the columns using VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=spark_data.columns, outputCol=\"features\",handleInvalid='keep')\n",
    "df = assembler.transform(spark_data).select(\"features\")\n",
    "\n",
    "# correlation will be in Dense Matrix\n",
    "correlation = Correlation.corr(df,\"features\",\"pearson\").collect()[0][0]\n",
    "\n",
    "# To convert Dense Matrix into DataFrame\n",
    "rows = correlation.toArray().tolist()\n",
    "df = spark.createDataFrame(rows,spark_data.columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d3861-c057-4290-87fb-a84e43de4b54",
   "metadata": {},
   "source": [
    "Now we have the correlation matrix. Rows of the correlation matrix are listed same as the columns of the correlation matrix.<br><br>\n",
    "We can see that `Power_Zone_1`, `Power_Zone_2` and `Power_Zone_3` have relatively stronger correlation. `Wind_Speed` and `Diffuse_Flows` have relatively strong negative correlation. Power zones 1 & 2  have relatively stronger correlation with `Hour`. Apart from these, rest of the predictors do not seem to be strongly correlated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f253ed6-8252-4bd7-a48b-55df7bc22c23",
   "metadata": {},
   "source": [
    "### One-way contigency tables for `Month` & `Hour`\n",
    "\n",
    "Now, let us look at the contigency tables for `Month` and `Hour` both individually (one-way contingency table) and together(two-way contingency table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20af31c8-2aa9-418c-842a-e620f1c46b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|dummy_Month|   1|  10|  11|  12|   2|   3|   4|   5|   6|   7|   8|   9|\n",
      "+-----------+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|          0|4014|4026|3877|3868|3588|4057|3893|3997|3913|4029|3999|3913|\n",
      "+-----------+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dummy column with all zeros\n",
    "spark_data1=spark_data.withColumn(\"dummy\", lit(0))\n",
    "\n",
    "# create one-way contigency table for the 12 months \n",
    "spark_data1.crosstab(\"dummy\",\"Month\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d629d5-bdfe-4d45-803a-4bf245764658",
   "metadata": {},
   "source": [
    "One-way contigency table above for `Month` shows the number of observations spread across 12 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "503846d5-ebb1-4335-830d-39ade9710891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|dummy_Hour| 0.0| 1.0|10.0|11.0|12.0|13.0|14.0|15.0|16.0|17.0|18.0|19.0| 2.0|20.0|21.0|22.0|23.0| 3.0| 4.0| 5.0| 6.0| 7.0| 8.0| 9.0|\n",
      "+----------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|         0|1950|1973|1955|1972|1979|1956|1971|1947|1950|1979|1955|1950|1973|1945|1976|1966|1968|1966|1986|1968|1992|1964|1957|1976|\n",
      "+----------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create one-way contigency table for 24 hours\n",
    "spark_data1.crosstab(\"dummy\",\"Hour\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447b3daa-419a-4be3-b2a4-dbf0c9bbfa8c",
   "metadata": {},
   "source": [
    "One-way contigency table above for `Hour` shows the number of observations spread across 24 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c42ee-b438-4727-966f-d588394e2c55",
   "metadata": {},
   "source": [
    "### Two-way contigency tables for `Month` & `Hour`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c13ebb5-7830-45a0-9cad-85366c3ce864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+---+----+----+----+----+----+----+----+----+----+----+---+----+----+----+----+---+---+---+---+---+---+---+\n",
      "|Month_Hour|0.0|1.0|10.0|11.0|12.0|13.0|14.0|15.0|16.0|17.0|18.0|19.0|2.0|20.0|21.0|22.0|23.0|3.0|4.0|5.0|6.0|7.0|8.0|9.0|\n",
      "+----------+---+---+----+----+----+----+----+----+----+----+----+----+---+----+----+----+----+---+---+---+---+---+---+---+\n",
      "|         5|164|166| 172| 159| 175| 166| 159| 172| 167| 171| 162| 160|163| 165| 174| 161| 162|171|174|164|172|163|167|168|\n",
      "|        10|171|168| 163| 171| 166| 161| 175| 165| 159| 176| 165| 162|175| 170| 173| 169| 167|172|162|163|170|164|168|171|\n",
      "|         1|161|165| 164| 174| 170| 165| 167| 163| 169| 170| 168| 162|168| 172| 166| 166| 165|162|167|168|168|168|172|174|\n",
      "|         6|160|164| 159| 169| 155| 167| 162| 158| 161| 161| 165| 162|169| 159| 156| 167| 166|165|167|167|163|167|160|164|\n",
      "|         9|164|161| 159| 161| 174| 159| 162| 168| 166| 169| 162| 163|160| 158| 163| 163| 167|164|167|158|162|159|162|162|\n",
      "|         2|142|151| 145| 141| 155| 154| 153| 146| 153| 152| 144| 148|147| 149| 151| 151| 145|148|156|147|157|153|147|153|\n",
      "|        12|154|159| 165| 167| 164| 156| 168| 157| 158| 153| 159| 160|167| 158| 159| 168| 156|167|164|164|165|165|156|159|\n",
      "|         7|172|169| 167| 168| 166| 173| 169| 162| 164| 172| 176| 172|161| 159| 163| 173| 167|170|167|174|168|164|165|168|\n",
      "|         3|174|171| 175| 168| 166| 173| 165| 171| 166| 163| 165| 170|167| 168| 175| 165| 172|168|166|170|173|170|166|170|\n",
      "|        11|159|168| 160| 160| 156| 158| 162| 163| 157| 166| 165| 160|168| 161| 162| 159| 164|154|165|153|162|167|164|164|\n",
      "|         8|168|169| 162| 171| 167| 167| 164| 166| 168| 165| 169| 168|171| 165| 169| 164| 173|165|168|171|165|162|165|157|\n",
      "|         4|161|162| 164| 163| 165| 157| 165| 156| 162| 161| 155| 163|157| 161| 165| 160| 164|160|163|169|167|162|165|166|\n",
      "+----------+---+---+----+----+----+----+----+----+----+----+----+----+---+----+----+----+----+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_data.crosstab(\"Month\",\"Hour\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d70ea5-cb7a-428e-8f65-86b99c2199f9",
   "metadata": {},
   "source": [
    "Two-way contigency table above shows the number of observations spread across month and hours. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba92ab6-6395-417a-a3ef-3fd9d38d6a03",
   "metadata": {},
   "source": [
    "### Means of all numeric variables grouped by `Month`\n",
    "\n",
    "Now, we will look at the means of all the numeric variables across twelve months.\n",
    "\n",
    "(You will notice that the summary is created in two parts. This is done only for the readability purposes and to make the results look clean.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7edb94a1-66c7-4834-a9c5-8b8d3afdfae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-----------------+-------------------+--------------------------+------------------+\n",
      "|Month|  avg(Temperature)|    avg(Humidity)|    avg(Wind_Speed)|avg(General_Diffuse_Flows)|avg(Diffuse_Flows)|\n",
      "+-----+------------------+-----------------+-------------------+--------------------------+------------------+\n",
      "|    1|12.734699053313424|68.25854758345791| 0.7022234678624802|        103.95965819631209| 69.79882635774754|\n",
      "|    2|12.656535117056842|66.49092530657751| 1.1139765886287718|        125.47113545150515|  92.3306151059088|\n",
      "|    3|14.584054720236646|71.11588365787539| 1.0060172541286598|        181.40171949716375| 93.15590460931698|\n",
      "|    5|20.301401050788073|68.60932199149357|   2.30747285464099|        274.50002601951394|122.76557593194893|\n",
      "|    4|16.414754687901336|75.40817621371694|  0.222989725147702|         157.7222427433853| 83.49453686103197|\n",
      "|    6| 22.13270636340399|68.76125990288782| 1.5613462816253483|         277.4345330948132|  103.227789164323|\n",
      "|    7|27.200593199304986|57.59948374286429|  4.641781831719927|         294.1120372300817| 75.41053760238238|\n",
      "|    8|25.740415103775955|66.02262065516375|  4.533251062765713|        227.17863515878855| 67.10584746186578|\n",
      "|    9|22.640564784053243|66.86830564784064|  2.947096089956558|         202.2016335292607| 49.07062202913375|\n",
      "|   10|20.476249379036258| 71.5240163934426| 2.7842208147044194|         115.8145561351204|46.628719324391646|\n",
      "|   11|16.819407531596593| 69.6383415011607|  1.258901728140313|        121.91511916430152| 62.87613257673453|\n",
      "|   12|13.283378490175783|69.23738624612214|0.25532445708376034|        100.02325051706269| 34.30538210961792|\n",
      "+-----+------------------+-----------------+-------------------+--------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_data.groupBy(\"Month\").avg(\"Temperature\", \"Humidity\", \"Wind_Speed\",\"General_Diffuse_Flows\",\"Diffuse_Flows\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fa53c73-9946-473a-a77d-c759dc205132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+------------------+------------------+\n",
      "|Month| avg(Power_Zone_1)| avg(Power_Zone_2)| avg(Power_Zone_3)|         avg(Hour)|\n",
      "+-----+------------------+------------------+------------------+------------------+\n",
      "|    1| 31052.98442787007|19407.916365649475| 17736.35168477324| 11.51270553064275|\n",
      "|    2|30973.863159715755|18774.586005972385|17309.707870419465|11.497491638795987|\n",
      "|    3|31162.869031165836|18459.612112786035|16945.462800258043|11.479171801824007|\n",
      "|    5| 32379.46046425318|19973.085386786082|17604.282564152116|11.466599949962472|\n",
      "|    4|31143.206765884937|17600.306571434827|18574.918338348314| 11.48214744413049|\n",
      "|    6|34573.227025573666|20649.034589747287| 20416.13009093538|11.456682852031689|\n",
      "|    7| 35805.53043592462| 24130.02818170255|   28175.034099032| 11.49565649044428|\n",
      "|    8| 36436.26165143788|24657.024551832943|24684.368960552885|11.506626656664166|\n",
      "|    9|33415.103455862554|20189.459836739075|14928.415530158953|  11.5323281369793|\n",
      "|   10| 32806.99279622949| 21457.89000130904| 13266.43733663137|11.486587183308496|\n",
      "|   11|28993.342195633202| 23229.11746956413| 12867.07310019397|11.492133092597369|\n",
      "|   12| 28959.10286604968| 23618.56451605991|11017.120312390387|11.452430196483972|\n",
      "+-----+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_data.groupBy(\"Month\").avg(\"Power_Zone_1\",\"Power_Zone_2\", \"Power_Zone_3\", \"Hour\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951bc711-e81a-4474-bcb9-eca7c0454b42",
   "metadata": {},
   "source": [
    "### Standard Deviation of all numeric variables grouped by `Month`\n",
    "\n",
    "Now, we will look at the standard deviation of all the numeric variables across twelve months.\n",
    "\n",
    "(You will notice that the summary is created in two parts. This is done only for the readability purposes and to make the results look clean.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d4b74bc-08cc-451b-b71d-90a02b7e465f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-----------------------------+------------------+---------------------+-------------------+\n",
      "|Month|stddev(Wind_Speed)|stddev(General_Diffuse_Flows)|  stddev(Humidity)|stddev(Diffuse_Flows)|stddev(Temperature)|\n",
      "+-----+------------------+-----------------------------+------------------+---------------------+-------------------+\n",
      "|    1|1.6117952014385526|           166.16470976772894| 12.15616997395505|     131.459171585262| 3.2406352202253177|\n",
      "|    2|1.9811568615798605|           206.73018017500877|12.411941927246787|   169.15551710161583|  2.619715133289512|\n",
      "|    3|1.9009817904056068|           260.14888917166206|13.918146099336553|   151.16792344997845| 3.7588517619095954|\n",
      "|    5|2.4083281081182726|           331.99889739474145|16.436022749116013|   171.58594340324657| 3.2999517830264136|\n",
      "|    4|0.8203433573472342|           246.17350131285014|14.312654855624078|   123.91235156483745|  2.806220593833569|\n",
      "|    6|2.2354105856986264|             328.277209820488|14.972905335787273|    143.4978962074959| 2.6897243987839956|\n",
      "|    7|1.1105390901773047|            331.7336077428813|18.849986112632102|    95.04451651686645| 3.8567073301651567|\n",
      "|    8|1.3008996273361944|            289.9061923113907|18.482550994912017|    90.66254000257548|  2.949887327569401|\n",
      "|    9|2.2934124256572472|            270.1725641702132| 15.99282606989947|    67.52353874901077| 2.8775846322752625|\n",
      "|   10|2.3987026068167046|            185.0432984170808|13.980791119840983|    69.42130815062337|  2.987292863085969|\n",
      "|   11| 2.075376528780303|           184.69202456862132|12.832010541598438|   123.14150471869303| 3.6832250625264473|\n",
      "|   12|0.8937554493017353|           161.70244063920802|13.739463409846879|    56.39356877719106|  3.348618746384049|\n",
      "+-----+------------------+-----------------------------+------------------+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_data.groupBy(\"Month\").agg({\"Temperature\":\"stddev\", \"Humidity\": \"stddev\", \n",
    "                                 \"Wind_Speed\": \"stddev\",\"General_Diffuse_Flows\": \"stddev\",\n",
    "                                 \"Diffuse_Flows\":\"stddev\"}).show()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab34b895-d496-47bf-a961-201a02e6c70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+------------------+\n",
      "|Month|stddev(Power_Zone_3)|stddev(Power_Zone_1)|stddev(Power_Zone_2)|      stddev(Hour)|\n",
      "+-----+--------------------+--------------------+--------------------+------------------+\n",
      "|    1|   4436.997404934671|   7402.323410683091|   4515.295696366688|6.8933864667013784|\n",
      "|    2|  4353.9759463094815|   6874.584790780019|   4390.391100519847| 6.900280948950495|\n",
      "|    3|    4256.76554626955|   6782.136538982632|   4185.117594550106| 6.934685602294895|\n",
      "|    5|   4353.394233957315|   6809.332811001886|  4182.5436720014095|6.9047355594893345|\n",
      "|    4|   4556.263192196545|   6496.700166744174|  3835.6293841116035|  6.91394782479513|\n",
      "|    6|   5596.702925763702|   7317.808097492525|   4465.664309494708| 6.933494099313091|\n",
      "|    7|    6913.95836073139|   6966.074191037386|    4968.51110111139| 6.927292532034068|\n",
      "|    8|   6520.955954895705|   7054.722275396859|   5163.442764556622| 6.950722974866449|\n",
      "|    9|  3425.8563203955623|    6471.36690049191|  4205.5042189416745| 6.922189515744973|\n",
      "|   10|  3087.8058176327845|  6479.1334820123275|   4612.671174818133| 6.944603798274296|\n",
      "|   11|   3509.671953716189|   5918.582563224674|   5452.892157923399| 6.933768382211351|\n",
      "|   12|  2840.3432089859934|   6176.465110237933|   5713.467068605237| 6.910305828988747|\n",
      "+-----+--------------------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_data.groupBy(\"Month\").agg({\"Power_Zone_1\":\"stddev\", \"Power_Zone_2\": \"stddev\", \n",
    "                                 \"Power_Zone_3\": \"stddev\",\"Hour\": \"stddev\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54716b68-b60d-49af-99bb-33bac0d4db2d",
   "metadata": {},
   "source": [
    "We have looked at the summary of the dataset.\n",
    "\n",
    "Now we will move to performing some transformations in Spark and use `MLlib` functions to make predictions. Transformations are needed before we input the data into a model. \n",
    "\n",
    "Here, we are fitting Elastic Net Model. In the fitting of linear or logistic regression models, the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods. We will tune our regularization parameters by using 5-fold cross validation.\n",
    "\n",
    "First, let us start with importing required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40316c58-38ea-4dc1-ac9b-34011b940255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.ml.feature import SQLTransformer, PCA, Binarizer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e63094-9387-434a-9a26-f58e4686937f",
   "metadata": {},
   "source": [
    "Now that we have imported the required libraries, we need to perform some transformations on the dataset. These transformations will then be added to the pipeline. \n",
    "\n",
    "Now, we will define a series of transformations before we fit them into a pipeline. The stages of the transformations are explained as follows:\n",
    "\n",
    "1) `SQLTransformer`: First, we will use `SQLTransformer` to obtain the variables that we will use in the analysis. We will use `Temperature`, `Humidity`, `Wind_Speed`, `General_Diffuse_Flows`, `Diffuse_Flows`, `Power_Zone_1`, `Power_Zone_2`, `Month`, and `Hour` as predictors and `Power_zone_3` as the response variable. In order to fit data using `MLlib`, we need to define our predictors and response variables as `features` and `label` respectively. Here, we will define `Power_zone_3` as `label` to meet this requirement.\n",
    "\n",
    "2) `Binarizer`: Since `Hour` variable denotes time and its value doesn't indicate the magnitude, we need to transform this variable using `Binarizer`. We will set a threshold of 6.5 to convert this variable.\n",
    "\n",
    "3) `OneHotEncoder`: `Month` variable denotes time-period and its value doesn't indicate the magnitude. We will use `Month` variable to transform it into one-hot encoded vector. The one-hot encoding represents each category as a binary vector with a single \"1\" in the position corresponding to the category index and \"0\" elsewhere. \n",
    "\n",
    "4) `VectorAssembler`: This stage is a preparatory stage for the next stage (PCA transformation) where the variables are required in the vector form. Here, the input variables will be `Temperature`, `Humidity`, `Wind_Speed`, `General_Diffuse_Flows`, and `Diffuse_Flows`, which then will be converted into a vector.\n",
    "\n",
    "5) `PCA`: This transformation method will train a model to project vectors to a lower dimensional space of the top k principal components.\n",
    "\n",
    "6) `VectorAssembler`: This stage is defined again to create `features` vector as a requirement of `MLlib`. This vector will be created from binarized `Hour`, encoded `Month` and `PCA_features` obtained from `PCA`.\n",
    "\n",
    "7) `LinearRegression`: Finally, we will create an instance for linear regression model that will be fit to the data. Also, we will create a parameter grid to define multiple values for regularizing parameters and elastic net parameters which will be tuned later and the best parameters will be used for predictions.\n",
    "\n",
    "Lastly, we will create a pipeline that incorporates above transformation stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6e1c1e7-14a6-4dc9-99b3-6fc3ca73ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLTransformer to get select variables from dataset\n",
    "sqlTrans = SQLTransformer(statement=\"SELECT Temperature, Humidity, Wind_Speed, General_Diffuse_Flows, \\\n",
    "                            Diffuse_Flows, Power_Zone_1, Power_Zone_2, Month, Hour, Power_Zone_3 as label FROM __THIS__\")\n",
    "\n",
    "# Binarizer for hour column\n",
    "binaryTrans = Binarizer(threshold = 6.5, inputCol = \"Hour\", outputCol = \"Hour_binarized\")\n",
    "\n",
    "# OneHotEncoder to transform Month variable into a one-hot encoded vector\n",
    "encoder = OneHotEncoder(inputCol=\"Month\", outputCol=\"Month_encode\")\n",
    "\n",
    "# VectorAssembler for vector being created for PCA transformation\n",
    "assembler1 = VectorAssembler(inputCols=[\"Temperature\", \"Humidity\", \"Wind_Speed\", \"General_Diffuse_Flows\", \"Diffuse_Flows\"], outputCol = \"Features1\")\n",
    "\n",
    "# PCA transformer on Features1\n",
    "pca = PCA(k=2, inputCol=\"Features1\", outputCol=\"PCA_features\")\n",
    "\n",
    "# VectorAssembler to get MLlib requisite 'features' column\n",
    "assembler2 = VectorAssembler(inputCols=[\"PCA_features\",\"Hour_binarized\",\"Power_Zone_1\",\"Power_Zone_2\",\"Month_encode\"], outputCol=\"features\")\n",
    "\n",
    "# Create Linear Regression model instance\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Define paramter grid for elastic net \n",
    "lr_paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.98, 0.99, 1])\\\n",
    "                                .addGrid(lr.elasticNetParam, [0, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.98, 0.99, 1]).build() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf372f8-a32b-4c54-943d-ccf158d88774",
   "metadata": {},
   "source": [
    "Here is the pipeline with above-mentioned transformation stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6967bed-5e61-415f-bf70-a077324ba6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline with the stages\n",
    "lr_pipeline = Pipeline(stages=[sqlTrans, binaryTrans, encoder, assembler1, pca, assembler2, lr ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b584b1c3-6a4c-4c1a-9bfb-1578468b4972",
   "metadata": {},
   "source": [
    "We have the pipeline ready!\n",
    "\n",
    "Now we will use this linear regression pipeline within the CV calculation. For this purpose, we will first define evaluator for cross-validation using RegressionEvaluator with metric rmse. This cross-validator will then be used to fit on the training dataset and the predictions will be made using the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcd7cf93-70b8-465d-8001-e3b6fa0be3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE for our Elastic Net model: 2147.097345981367\n"
     ]
    }
   ],
   "source": [
    "# RegressionEvaluator for cross-validation\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\")\n",
    "\n",
    "# Create the CrossValidator with 5 folds\n",
    "crossval = CrossValidator(estimator=lr_pipeline,\n",
    "                                estimatorParamMaps=lr_paramGrid,\n",
    "                                evaluator=evaluator,\n",
    "                                numFolds=5)\n",
    "\n",
    "# Fit the CrossValidator model on the train data & make predictions on test data\n",
    "cvModel = crossval.fit(spark_data)\n",
    "predictions = cvModel.transform(spark_data)\n",
    "\n",
    "# Calculate the training RMSE\n",
    "rmse = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\").evaluate(predictions)\n",
    "print(f\"Training RMSE for our Elastic Net model: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d75d6-e633-48eb-bc4d-814c5603a46a",
   "metadata": {},
   "source": [
    "We have the training RMSE value for the model. Since we do not have any other models to compare, it is difficult to determine how our model performed in terms of accuracy. However, goals of this project are to be able to use the streaming data to make predictions, therefore, we are not evaluating model performance for this once. \n",
    "\n",
    "Now let us create a `residual` column that is calculated by subtracting `prediction` column from `label` (Power Zone 3 values) column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36d3d9b5-433d-4f99-be84-a94a27735e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+---------------------+-------------+------------+------------+-----+----+-----------+--------------+--------------+--------------------+--------------------+--------------------+-----------------+------------------+\n",
      "|Temperature|Humidity|Wind_Speed|General_Diffuse_Flows|Diffuse_Flows|Power_Zone_1|Power_Zone_2|Month|Hour|      label|Hour_binarized|  Month_encode|           Features1|        PCA_features|            features|       prediction|          residual|\n",
      "+-----------+--------+----------+---------------------+-------------+------------+------------+-----+----+-----------+--------------+--------------+--------------------+--------------------+--------------------+-----------------+------------------+\n",
      "|      6.559|    73.8|     0.083|                0.051|        0.119|  34055.6962| 16128.87538|    1| 0.0|20240.96386|           0.0|(12,[1],[1.0])|[6.559,73.8,0.083...|[1.79440486365695...|(17,[0,1,3,4,6],[...|20880.29948217905|-639.3356221790491|\n",
      "+-----------+--------+----------+---------------------+-------------+------------+------------+-----+----+-----------+--------------+--------------+--------------------+--------------------+--------------------+-----------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a residual column\n",
    "predictions.withColumn(\"residual\", col(\"label\") - col(\"prediction\")).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7c868d-70f3-42b3-8b05-7a852cbbd655",
   "metadata": {},
   "source": [
    "This concludes the first part of the our project where we made predictions on the batch data. Now we will utilize these functions on streaming data that will be generated randomly from the existing data and will be fed to the functions iteratively.\n",
    "\n",
    "Let's move on to the model fitting on streaming data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa7ff19-7945-4287-adc1-2011ac722067",
   "metadata": {},
   "source": [
    "## Part-2: Streaming Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ffd0d-e641-4416-a736-7c5b57d8d289",
   "metadata": {},
   "source": [
    "In this part of the project, we will use streaming queries to run the functions that we used for the batch data. Let's first start with importing required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d894d14-ceb8-46b5-af64-f8a1b288d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType, DoubleType, FloatType\n",
    "from pyspark.sql.functions import explode, split, col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e44e9d-d16f-45b3-86d9-b7a88d0cafff",
   "metadata": {},
   "source": [
    "#### Reading a stream\n",
    "\n",
    "Our first task is to initialize the Spark session (even though we started the Spark session in the first part, for the sake of individuality, we have started the session again--not necessary, however). Now, we will define the schema same as did in the first part. Finally, let's create a stream object that reads in the streamed data in the form of .csv files into a folder named `csv_files_folder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3879d58-e582-4096-a5b4-10e9df022315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading a stream\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Power Consumption Streaming\").getOrCreate()\n",
    "\n",
    "# Setup my schema (same as model fitting part)\n",
    "my_schema = StructType([\n",
    "    StructField(\"Temperature\", DoubleType(), True),\n",
    "    StructField(\"Humidity\", DoubleType(), True),\n",
    "    StructField(\"Wind_Speed\", DoubleType(), True),\n",
    "    StructField(\"General_Diffuse_Flows\", DoubleType(), True),\n",
    "    StructField(\"Diffuse_Flows\", DoubleType(), True),\n",
    "    StructField(\"Power_Zone_1\", DoubleType(), True),\n",
    "    StructField(\"Power_Zone_2\", DoubleType(), True),\n",
    "    StructField(\"Power_Zone_3\", DoubleType(), True),\n",
    "    StructField(\"Month\", IntegerType(), True),\n",
    "    StructField(\"Hour\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "# Setup the readStream to read in a stream into a folder\n",
    "stream_df = spark.readStream.schema(my_schema).option(\"header\",\"true\").csv(\"csv_files_folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26899d57-1327-45f1-9e35-b891e1e688f0",
   "metadata": {},
   "source": [
    "##### Transform/Aggregation\n",
    "\n",
    "Now that we have set up our schema and readStream, we will perform our transformation/aggregation in two steps:\n",
    "\n",
    "- First, we will first use the model transfomer(`cvModel`) from the first part on the data being read in a stream. In addition to this, we will create a `residual` column (`label` minus `prediction`). This step gives us our first dataframe.\n",
    "\n",
    "- Second, we will create a second pipeline that includes all the stages(`SQLTransfromer`, `binarizer`, `OneHotEncoder`, `VectorAssembler`, & `PCA`) done above except for the linear regression transformation. The difference here is that we will return all of the columns from the original data and the transformations done (including the `label` and `features` columns). This step gives us our second dataframe. For this, we will fit the pipeline on the original data obtained from the UCI repository and then we will perform transformations using streaming data that will be produced separately.\n",
    "\n",
    "Both of the resulting dataframes will then be joined by using `.join()` method. We will use inner join to selects records that have matching values based on `label` variable in both the tables.\n",
    "\n",
    "Now let's do the first step to get our first dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39bb2733-8dfe-4391-ba67-fef7d72f1dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform/Aggregation Step-1\n",
    "\n",
    "# Use the model transformer to obtain predictions from the incoming data\n",
    "stream_predictions = cvModel.transform(stream_df)\n",
    "\n",
    "# Create a residual column (label minus prediction)\n",
    "stream_residuals = stream_predictions.withColumn(\"residual\", col(\"label\") - col(\"prediction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c8aa6f-19b6-495c-9621-df90edb0d7ba",
   "metadata": {},
   "source": [
    "We have created our first dataframe `stream_residuals`!\n",
    "\n",
    "Let's do the second step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2964bb74-61c0-4361-89e7-844f22b9d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform/Aggregation Step-2\n",
    "\n",
    "# SQL Transformer to get all the vector and non-vector variables from dataset\n",
    "sqlTrans2= SQLTransformer(statement=\"SELECT features, Month_encode, Hour_binarized, Temperature, Humidity, Wind_Speed, General_Diffuse_Flows, \\\n",
    "                            Diffuse_Flows, Power_Zone_1, Power_Zone_2, Month, Hour, label FROM __THIS__\")\n",
    "\n",
    "# Second pipeline excluding model fitting\n",
    "pipeline2 = Pipeline(stages=[sqlTrans, binaryTrans, encoder, assembler1, pca, assembler2, sqlTrans2])\n",
    "\n",
    "# Train the pipeline on the original data that is our batch data from part 1\n",
    "pipeline2_model = pipeline2.fit(spark_data)\n",
    "\n",
    "# Transform the streaming data with the second pipeline model\n",
    "transformed_stream = pipeline2_model.transform(stream_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78d4cb8-7d60-464a-9cc8-78d3184ed711",
   "metadata": {},
   "source": [
    "We have the second dataframe `transformed_stream`!\n",
    "\n",
    "Let's join these two dataframes using `inner` join!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f199af19-962b-4a9f-89d4-959daf1b00b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two streaming DataFrames on the 'label' column\n",
    "joined_stream = stream_residuals.join(transformed_stream, on=\"label\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1247635-00c9-4be6-87a0-96f8f0c9875c",
   "metadata": {},
   "source": [
    "Now we have our final dataframe from the streaming data.\n",
    "\n",
    "We need to write this final stream into the console to start the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12021799-2a89-4d26-8afd-035c3c478a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the joined_stream DataFrame to the console\n",
    "query = joined_stream.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "\n",
    "# Start the query and wait for it to finish\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e968dae7-9874-421b-a464-79b20b2df01e",
   "metadata": {},
   "source": [
    "The above query will be executed on the streaming data that is produced iteratively from the `power_streaming_data.csv`. The output from this query will be shown in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf28e26-b85e-4677-8338-5efd5bded00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to stop the query once we are done executing\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48c677a-4dcc-4533-8d1a-68516e4c33e2",
   "metadata": {},
   "source": [
    "### Produce Data\n",
    "\n",
    "**Below-mentioned code will be submitted in Python console in another window to simulate the idea of getting new data in.**\n",
    "\n",
    "Here we will produce our streaming data by sampling three rows from the regular dataframe `power_streaming_data.csv` and output those rows in the .csv file form and store those files in the `csv_files_folder`. These files will then be used in model fitting and creating predictions. The predictions will be available on the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b814e442-beac-4e16-8dcb-6ecfccb5cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Read the streaming data into a pandas DataFrame\n",
    "streaming_data = pd.read_csv(\"power_streaming_data.csv\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir = \"csv_files_folder\"\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for i in range(50):\n",
    "    # Randomly sample three rows from the DataFrame\n",
    "    sample_data = streaming_data.sample(n=3)\n",
    "\n",
    "    # Write the sampled rows to a .csv file in the cv_files_folder without indices\n",
    "    sample_data.to_csv(os.path.join(output_dir, f\"sample_{i}.csv\"), index=False)\n",
    "\n",
    "    # Pause for 10 seconds before the next iteration\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458de082-561b-407f-8ee8-436b3d2695fd",
   "metadata": {},
   "source": [
    "This concludes our model fitting part on the streaming data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
